{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import requests\n",
    "import re\n",
    "import unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_movies_info(name_archive: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function reads a file containing movie information and returns a dataframe\n",
    "    containing the movie information.\n",
    "\n",
    "    Args:\n",
    "        name_archive: name of the xlsx archive with the movie info\n",
    "\n",
    "    Returns:\n",
    "        movies_df: dataframe with the movie info\n",
    "    \"\"\"\n",
    "    movies_df = pd.read_excel(name_archive)\n",
    "    # Convert stringified lists to actual Python lists\n",
    "    movies_df['directors'] = movies_df['directors'].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else []\n",
    "    )\n",
    "\n",
    "    # Now extract names and URLs\n",
    "    movies_df['director_names'] = movies_df['directors'].apply(\n",
    "        lambda x: ', '.join([d.get('name', '') for d in x]) if isinstance(x, list) else ''\n",
    "    )\n",
    "\n",
    "    movies_df['director_urls'] = movies_df['directors'].apply(\n",
    "        lambda x: ', '.join([d.get('url', '') for d in x]) if isinstance(x, list) else ''\n",
    "    )\n",
    "    return movies_df\n",
    "\n",
    "def parsing_directors_URL(movies_df: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    This function receives the dataframe with the movie info and parse the\n",
    "    director urls and save it into a list.\n",
    "\n",
    "    Args:\n",
    "        movies_df: dataframe with the movie info\n",
    "\n",
    "    Returns:\n",
    "        director_urls: list with the directors urls\n",
    "    \"\"\"\n",
    "    # Get all non-empty director_urls, split by comma, and flatten\n",
    "    director_urls = movies_df[movies_df['director_urls'] != '']['director_urls'] \\\n",
    "        .str.split(', ') \\\n",
    "        .explode() \\\n",
    "        .tolist()\n",
    "    return director_urls\n",
    "\n",
    "def get_director_raw_info(url: str) -> tuple[list[Tag], str]:\n",
    "    \"\"\"\n",
    "    This function retrieves the raw HTML content of a director's IMDb page\n",
    "    and extracts the raw info block and the director's name.\n",
    "\n",
    "    Args:\n",
    "        url: IMDb URL of the director's page\n",
    "\n",
    "    Returns:\n",
    "        dir_info_raw: list of HTML elements containing the raw data about the director\n",
    "        name: name of the director\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    dir_info_raw = soup.find_all('li', {'data-testid': True})\n",
    "\n",
    "    # Extracting director name from the title\n",
    "    title_tag = soup.find('title')\n",
    "    name = title_tag.get_text(strip=True).split('-')[0].strip() if title_tag else 'Nombre no encontrado'\n",
    "\n",
    "    return dir_info_raw, name\n",
    "\n",
    "def get_director_structured_info(name: str, item: Tag) -> dict:\n",
    "    \"\"\"\n",
    "    This function takes the raw HTML element of a director's credit and returns\n",
    "    a structured dictionary with relevant information.\n",
    "\n",
    "    Args:\n",
    "        name: name of the director\n",
    "        item: raw HTML item from the IMDb page\n",
    "\n",
    "    Returns:\n",
    "        response: structured information including name, category, movie name, rating, and URL;\n",
    "                  if an error occurs, a dictionary with an error message is returned\n",
    "    \"\"\"\n",
    "    try:\n",
    "        category = item.get(\"data-testid\", default=\"no_category\")\n",
    "\n",
    "        if not category.startswith(\"cred\"):\n",
    "            return {\"error\": \"unexpected category\"}\n",
    "\n",
    "        match = re.search(r'_(.*?)_', category)\n",
    "        category_cleaned = unidecode.unidecode(match.group(1))\n",
    "\n",
    "        info = item.find('a', {'aria-label': True})\n",
    "        movie_name = info[\"aria-label\"]\n",
    "        url = f\"https://www.imdb.com{info.get('href')}\"\n",
    "\n",
    "        rating_span = item.find('span', class_='ipc-rating-star--rating')\n",
    "        rating = rating_span.get_text(strip=True) if rating_span else 'N/A'\n",
    "\n",
    "        response = {\n",
    "            \"name\": name,\n",
    "            \"category\": category_cleaned,\n",
    "            \"movie_name\": movie_name,\n",
    "            \"rating\": rating,\n",
    "            \"url\": url,\n",
    "        }\n",
    "        return response\n",
    "    except Exception as ex:\n",
    "        return {\"error\": f\"{ex}\"}\n",
    "\n",
    "def obtaining_info_per_url(urls: list[str]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    This function loops through a list of director IMDb URLs, parses the raw and structured\n",
    "    information, and aggregates it into a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        urls: list of IMDb URLs for directors\n",
    "\n",
    "    Returns:\n",
    "        all_structured_data: list of dictionaries containing structured movie info for each director\n",
    "    \"\"\"\n",
    "    all_structured_data = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            raw_info, director_name = get_director_raw_info(url)\n",
    "\n",
    "            structured_data = list(\n",
    "                filter(\n",
    "                    lambda item: \"error\" not in item,\n",
    "                    map(lambda item: get_director_structured_info(director_name, item), raw_info)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            all_structured_data.extend(structured_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL {url}: {e}\")\n",
    "    return all_structured_data\n",
    "\n",
    "def scrape_imdb_awards(imdb_id):\n",
    "    url = f\"https://www.imdb.com/name/{imdb_id}/awards\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    tables = soup.find_all(\"table\", class_=\"awards\")\n",
    "    data = []\n",
    "    current_award = None\n",
    "\n",
    "    for table in tables:\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            if \"award_category\" in row.get(\"class\", []):\n",
    "                current_award = row.get_text(strip=True)\n",
    "            elif \"award_description\" in row.get(\"class\", []):\n",
    "                outcome = row.find(\"td\", class_=\"title_award_outcome\").get_text(strip=True)\n",
    "                outcome_type = row.find(\"b\").get_text(strip=True) if row.find(\"b\") else \"\"\n",
    "                desc = row.find(\"td\", class_=\"award_description\").get_text(strip=True)\n",
    "                title_tag = row.find(\"td\", class_=\"award_description\").find(\"a\")\n",
    "                title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "                year_tag = row.find(\"span\", class_=\"award_year\")\n",
    "                year = year_tag.get_text(strip=True) if year_tag else \"\"\n",
    "\n",
    "                data.append({\n",
    "                    \"award\": current_award,\n",
    "                    \"outcome\": outcome_type,\n",
    "                    \"category_and_description\": desc,\n",
    "                    \"title\": title,\n",
    "                    \"year\": year\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = parsing_movies_info(\"movies.xlsx\")\n",
    "urls = parsing_directors_URL(movies_df)\n",
    "all_structured_data = obtaining_info_per_url(urls)\n",
    "df_directors = pd.DataFrame(all_structured_data)\n",
    "df_directors['imdb_id'] = df_directors['director_url'].apply(\n",
    "    lambda url: re.search(r'nm\\d+', url).group(0) if pd.notnull(url) else None\n",
    ")\n",
    "\n",
    "all_awards = []\n",
    "for _, row in df_directors.iterrows():\n",
    "    name = row['director_name']\n",
    "    imdb_id = row['imdb_id']\n",
    "\n",
    "    if pd.notnull(imdb_id):\n",
    "        print(f\"üîç Scraping {name} ({imdb_id})...\")\n",
    "        try:\n",
    "            df = scrape_imdb_awards(imdb_id)\n",
    "            df[\"director_name\"] = name\n",
    "            all_awards.append(df)\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error with {name}: {e}\")\n",
    "\n",
    "# Combine all individual dataframes\n",
    "combined_awards_df = pd.concat(all_awards, ignore_index=True)\n",
    "\n",
    "df_directors.to_csv(\"./outputs/directors_info.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
