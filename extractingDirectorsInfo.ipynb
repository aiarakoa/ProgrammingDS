{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import requests\n",
    "import re\n",
    "import unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse, urlunparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Awards from Directors URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_movies_info(name_archive: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function reads a file containing movie information and returns a dataframe\n",
    "    containing the movie information.\n",
    "\n",
    "    Args:\n",
    "        name_archive: name of the xlsx archive with the movie info\n",
    "\n",
    "    Returns:\n",
    "        movies_df: dataframe with the movie info\n",
    "    \"\"\"\n",
    "    movies_df = pd.read_csv(name_archive)\n",
    "    # Convert stringified lists to actual Python lists\n",
    "    movies_df['directors'] = movies_df['directors'].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else []\n",
    "    )\n",
    "\n",
    "    # Now extract names and URLs\n",
    "    movies_df['director_names'] = movies_df['directors'].apply(\n",
    "        lambda x: ', '.join([d.get('name', '') for d in x]) if isinstance(x, list) else ''\n",
    "    )\n",
    "\n",
    "    movies_df['director_urls'] = movies_df['directors'].apply(\n",
    "        lambda x: ', '.join([d.get('url', '') for d in x]) if isinstance(x, list) else ''\n",
    "    )\n",
    "    return movies_df\n",
    "\n",
    "def parsing_directors_URL(movies_df: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    This function receives the dataframe with the movie info and parse the\n",
    "    director urls and save it into a list.\n",
    "\n",
    "    Args:\n",
    "        movies_df: dataframe with the movie info\n",
    "\n",
    "    Returns:\n",
    "        director_urls: list with the directors urls\n",
    "    \"\"\"\n",
    "    # Get all non-empty director_urls, split by comma, and flatten\n",
    "    director_urls = movies_df[movies_df['director_urls'] != '']['director_urls'] \\\n",
    "        .str.split(', ') \\\n",
    "        .explode() \\\n",
    "        .tolist()\n",
    "    return director_urls\n",
    "\n",
    "def get_director_raw_info(url: str) -> tuple[list[Tag], str]:\n",
    "    \"\"\"\n",
    "    This function retrieves the raw HTML content of a director's IMDb page\n",
    "    and extracts the raw info block and the director's name.\n",
    "\n",
    "    Args:\n",
    "        url: IMDb URL of the director's page\n",
    "\n",
    "    Returns:\n",
    "        dir_info_raw: list of HTML elements containing the raw data about the director\n",
    "        name: name of the director\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    dir_info_raw = soup.find_all('li', {'data-testid': True})\n",
    "\n",
    "    # Extracting director name from the title\n",
    "    title_tag = soup.find('title')\n",
    "    name = title_tag.get_text(strip=True).split('-')[0].strip() if title_tag else 'Nombre no encontrado'\n",
    "\n",
    "    return dir_info_raw, name\n",
    "\n",
    "def get_director_structured_info(name: str, item: Tag, director_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    This function takes the raw HTML element of a director's credit and returns\n",
    "    a structured dictionary with relevant information.\n",
    "\n",
    "    Args:\n",
    "        name: name of the director\n",
    "        item: raw HTML item from the IMDb page\n",
    "\n",
    "    Returns:\n",
    "        response: structured information including name, category, movie name, rating, and URL;\n",
    "                  if an error occurs, a dictionary with an error message is returned\n",
    "    \"\"\"\n",
    "    try:\n",
    "        category = item.get(\"data-testid\", default=\"no_category\")\n",
    "\n",
    "        if not category.startswith(\"cred\"):\n",
    "            return {\"error\": \"unexpected category\"}\n",
    "\n",
    "        match = re.search(r'_(.*?)_', category)\n",
    "        category_cleaned = unidecode.unidecode(match.group(1))\n",
    "\n",
    "        info = item.find('a', {'aria-label': True})\n",
    "        movie_name = info[\"aria-label\"]\n",
    "        url = f\"https://www.imdb.com{info.get('href')}\"\n",
    "\n",
    "        rating_span = item.find('span', class_='ipc-rating-star--rating')\n",
    "        rating = rating_span.get_text(strip=True) if rating_span else 'N/A'\n",
    "\n",
    "        response = {\n",
    "            \"name\": name,\n",
    "            \"category\": category_cleaned,\n",
    "            \"movie_name\": movie_name,\n",
    "            \"rating\": rating,\n",
    "            \"movie_url\": url,\n",
    "            \"director_url\": director_url\n",
    "        }\n",
    "        return response\n",
    "    except Exception as ex:\n",
    "        return {\"error\": f\"{ex}\"}\n",
    "\n",
    "def obtaining_info_per_url(urls: list[str]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    This function loops through a list of director IMDb URLs, parses the raw and structured\n",
    "    information, and aggregates it into a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        urls: list of IMDb URLs for directors\n",
    "\n",
    "    Returns:\n",
    "        all_structured_data: list of dictionaries containing structured movie info for each director\n",
    "    \"\"\"\n",
    "    all_structured_data = []\n",
    "\n",
    "    for url in tqdm(urls):\n",
    "        try:\n",
    "            raw_info, director_name = get_director_raw_info(url)\n",
    "\n",
    "            structured_data = list(\n",
    "                filter(\n",
    "                    lambda item: \"error\" not in item,\n",
    "                    map(lambda item: get_director_structured_info(director_name, item, url), raw_info)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            all_structured_data.extend(structured_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL {url}: {e}\")\n",
    "    return all_structured_data\n",
    "\n",
    "def scrapeDirectorAwards(director_url: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Scrapes a director's IMDb awards page and returns structured information about each award.\n",
    "\n",
    "    This function navigates to the director's IMDb awards subpage, parses the award listings,\n",
    "    and extracts relevant metadata such as year, award name, category, and the title associated\n",
    "    with the award (e.g., a movie or show). If the page cannot be accessed or parsed, it returns\n",
    "    an empty list.\n",
    "\n",
    "    Args:\n",
    "        director_url: The base URL to the director's IMDb profile page (e.g., \"https://www.imdb.com/name/nm0000001\").\n",
    "\n",
    "    Returns:\n",
    "        results: A list of dictionaries containing the director's name, IMDb URL, award result (e.g., winner or nominee),\n",
    "                 award name, category, and the title associated with the award. Returns an empty list if scraping fails.\n",
    "    \"\"\"\n",
    "    awards_url = director_url.rstrip('/') + \"/awards\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(awards_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {awards_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    award_divs = soup.select(\".ipc-metadata-list-summary-item__tc\")\n",
    "\n",
    "    # Extract director's name from the <title> tag or header\n",
    "    director_name = \"\"\n",
    "    try:\n",
    "        director_name_tag = soup.select_one(\"title\")\n",
    "        if director_name_tag:\n",
    "            director_name = director_name_tag.text.split(\"-\")[0].strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    results = []\n",
    "    for award_div in award_divs:\n",
    "        try:\n",
    "            main_award = award_div.select_one(\"a.ipc-metadata-list-summary-item__t\")\n",
    "            if not main_award:\n",
    "                continue\n",
    "            year_result = main_award.contents[0].strip()\n",
    "            award_name = main_award.select_one(\"span\").text.strip()\n",
    "\n",
    "            category_tag = award_div.select_one(\".awardCategoryName\")\n",
    "            category = category_tag.text.strip() if category_tag else \"\"\n",
    "\n",
    "            title_tag = award_div.select_one(\".ipc-metadata-list-summary-item__stl a\")\n",
    "            title = title_tag.text.strip() if title_tag else \"\"\n",
    "\n",
    "            results.append({\n",
    "                \"director_name\": director_name,\n",
    "                \"director_url\": director_url,\n",
    "                \"year_result\": year_result,\n",
    "                \"award_name\": award_name,\n",
    "                \"category\": category,\n",
    "                \"title\": title\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to parse one award block: {e}\")\n",
    "            continue\n",
    "\n",
    "    return results\n",
    "\n",
    "def standardize_imdb_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Standardizes an IMDb URL by removing locale-specific segments and stripping query parameters or fragments.\n",
    "\n",
    "    This function ensures that IMDb URLs are consistent by removing regional prefixes (e.g., '/es-es/')\n",
    "    from the path and eliminating any trailing query strings or fragments.\n",
    "\n",
    "    Args:\n",
    "        url: The original IMDb URL, potentially containing locale info, query parameters, or fragments.\n",
    "\n",
    "    Returns:\n",
    "        A cleaned IMDb URL with a standardized path and no query or fragment components.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    # Remove '/es-es/' if present\n",
    "    clean_path = parsed.path.replace(\"/es-es/\", \"/\")\n",
    "    # Ensure only the base URL, without query or fragment\n",
    "    return urlunparse(parsed._replace(path=clean_path, query=\"\", fragment=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1130 [00:00<?, ?it/s]0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "100%|██████████| 1130/1130 [33:36<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "movies_df = parsing_movies_info(\"outputs/movies_all_countries.csv\")\n",
    "urls = parsing_directors_URL(movies_df)\n",
    "all_structured_data = obtaining_info_per_url(urls)\n",
    "df_directors = pd.DataFrame(all_structured_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_directors['movie_url'] = df_directors['movie_url'].apply(standardize_imdb_url)\n",
    "df_directors['director_url'] = df_directors['director_url'].apply(standardize_imdb_url)\n",
    "df_cleaned = (\n",
    "    df_directors\n",
    "    .groupby(['name', 'movie_name', 'movie_url', 'director_url', 'rating'], as_index=False)\n",
    "    .agg({'category': lambda x: list(x)})\n",
    "    .rename(columns={'category': 'categories'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['url'] = movies_df['url'].apply(standardize_imdb_url)\n",
    "movies_df_dropped = movies_df.drop(['title', 'director_names', 'director_urls'], axis=1)\n",
    "movies_df_dropped = movies_df_dropped.rename(columns={'url': 'movie_url'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_cleaned.merge(movies_df_dropped, on='movie_url', how='left')\n",
    "df_merged = df_merged.drop(['awards', 'directors', 'name'], axis=1)\n",
    "df_merged = df_merged.rename(columns={'categories': 'participation_categories'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1130/1130 [33:40<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "all_awards = pd.DataFrame()\n",
    "for url in tqdm(urls):\n",
    "    awards_data = scrapeDirectorAwards(url)\n",
    "    if awards_data:  # Only proceed if something was scraped\n",
    "        awards_df = pd.DataFrame(awards_data)\n",
    "        all_awards = pd.concat([all_awards, awards_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_awards['director_url'] = all_awards['director_url'].apply(standardize_imdb_url)\n",
    "all_awards_cleaned = (\n",
    "    all_awards\n",
    "    .groupby(['director_name', 'director_url', 'title', 'year_result', 'award_name'], as_index=False)\n",
    "    .agg({'category': lambda x: list(x)})\n",
    ")\n",
    "all_awards_cleaned = all_awards_cleaned.rename(columns={'title': 'movie_name',\n",
    "                                                        'category':'award_categories'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = all_awards_cleaned.merge(df_merged, how='left', on=['director_url', 'movie_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22616 entries, 0 to 22615\n",
      "Data columns (total 17 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   director_name             22616 non-null  object \n",
      " 1   director_url              22616 non-null  object \n",
      " 2   movie_name                22616 non-null  object \n",
      " 3   year_result               22616 non-null  object \n",
      " 4   award_name                22616 non-null  object \n",
      " 5   award_categories          22616 non-null  object \n",
      " 6   movie_url                 17870 non-null  object \n",
      " 7   rating                    17870 non-null  object \n",
      " 8   participation_categories  17870 non-null  object \n",
      " 9   country                   3491 non-null   object \n",
      " 10  imdbRating                3491 non-null   object \n",
      " 11  imdbVotes                 3491 non-null   object \n",
      " 12  metascore                 2982 non-null   float64\n",
      " 13  budget                    352 non-null    object \n",
      " 14  domesticGross             1199 non-null   object \n",
      " 15  worldwideGross            1167 non-null   object \n",
      " 16  thespians                 3491 non-null   object \n",
      "dtypes: float64(1), object(16)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('./outputs/Q3/directors_info.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
